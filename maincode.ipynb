{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import datetime\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import multiprocessing as mp\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Col1', 'Col2']\n",
      "___________--\n",
      "      Col2\n",
      "Col1      \n",
      "0      347\n",
      "1       16\n",
      "2        9\n",
      "3       16\n",
      "4        9\n",
      "5       12\n",
      "6        5\n",
      "7       19\n",
      "8        7\n",
      "9       55\n",
      "10       9\n",
      "13      30\n",
      "14      14\n",
      "16       8\n",
      "17      12\n",
      "19      14\n",
      "20      12\n",
      "21      62\n",
      "22       9\n",
      "23      16\n",
      "24      15\n",
      "25      65\n",
      "26      62\n",
      "27       4\n",
      "28      11\n",
      "29      11\n",
      "30      15\n",
      "31      19\n",
      "32       5\n",
      "33       1\n",
      "...    ...\n",
      "3991     1\n",
      "3992     2\n",
      "3993     3\n",
      "3994     8\n",
      "3995     5\n",
      "3996     2\n",
      "3997     7\n",
      "3998     9\n",
      "3999     2\n",
      "4000     5\n",
      "4001     1\n",
      "4002     4\n",
      "4003     3\n",
      "4004     6\n",
      "4006     1\n",
      "4007     2\n",
      "4009     3\n",
      "4011     1\n",
      "4013     3\n",
      "4014     6\n",
      "4016     1\n",
      "4017     3\n",
      "4018     2\n",
      "4019     2\n",
      "4020     5\n",
      "4021     2\n",
      "4023     4\n",
      "4026     1\n",
      "4027     3\n",
      "4031     1\n",
      "\n",
      "[3663 rows x 1 columns]\n",
      "aaa\n",
      "      Col2\n",
      "Col1      \n",
      "107   1043\n",
      "1684   778\n",
      "1912   748\n",
      "3437   542\n",
      "0      347\n",
      "348    225\n",
      "1941   215\n",
      "1985   207\n",
      "483    191\n",
      "1917   189\n",
      "1943   187\n",
      "1938   185\n",
      "1983   185\n",
      "1946   183\n",
      "1993   183\n",
      "1962   179\n",
      "2047   174\n",
      "686    170\n",
      "1086   168\n",
      "1971   165\n",
      "1979   164\n",
      "1984   164\n",
      "2030   164\n",
      "925    163\n",
      "1126   163\n",
      "1966   163\n",
      "2059   162\n",
      "2078   162\n",
      "1577   161\n",
      "2073   161\n",
      "...    ...\n",
      "2074   130\n",
      "2081   129\n",
      "2172   129\n",
      "1076   126\n",
      "1211   126\n",
      "2188   126\n",
      "1107   125\n",
      "2032   125\n",
      "2139   125\n",
      "917    124\n",
      "1184   124\n",
      "2229   124\n",
      "136    123\n",
      "896    123\n",
      "1997   123\n",
      "2218   123\n",
      "1925   122\n",
      "2124   122\n",
      "2184   122\n",
      "1146   121\n",
      "2108   121\n",
      "2240   121\n",
      "2133   120\n",
      "376    119\n",
      "980    119\n",
      "1920   119\n",
      "2201   119\n",
      "1718   118\n",
      "1953   118\n",
      "2063   118\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "Col1\n",
      "107     1043\n",
      "1684     778\n",
      "1912     748\n",
      "3437     542\n",
      "0        347\n",
      "348      225\n",
      "1941     215\n",
      "1985     207\n",
      "483      191\n",
      "1917     189\n",
      "1943     187\n",
      "1938     185\n",
      "1983     185\n",
      "1946     183\n",
      "1993     183\n",
      "1962     179\n",
      "2047     174\n",
      "686      170\n",
      "1086     168\n",
      "1971     165\n",
      "1979     164\n",
      "1984     164\n",
      "2030     164\n",
      "925      163\n",
      "1126     163\n",
      "1966     163\n",
      "2059     162\n",
      "2078     162\n",
      "1577     161\n",
      "2073     161\n",
      "        ... \n",
      "2074     130\n",
      "2081     129\n",
      "2172     129\n",
      "1076     126\n",
      "1211     126\n",
      "2188     126\n",
      "1107     125\n",
      "2032     125\n",
      "2139     125\n",
      "917      124\n",
      "1184     124\n",
      "2229     124\n",
      "136      123\n",
      "896      123\n",
      "1997     123\n",
      "2218     123\n",
      "1925     122\n",
      "2124     122\n",
      "2184     122\n",
      "1146     121\n",
      "2108     121\n",
      "2240     121\n",
      "2133     120\n",
      "376      119\n",
      "980      119\n",
      "1920     119\n",
      "2201     119\n",
      "1718     118\n",
      "1953     118\n",
      "2063     118\n",
      "Name: Col2, Length: 100, dtype: int64\n",
      "['A', 'Col2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "path = os.getcwd()\n",
    "df = pd.read_table('/home/akarsh/Documents/fall_2020_course_documents/pattern_recognition/PR_project/facebook_combined.txt', delim_whitespace=True, names=('A','B'),dtype={'A': np.int64,'B': np.int64})#df = pd.read_excel(file_name, sheet_name=None)\n",
    "df.columns=['Col1','Col2']\n",
    "#print(list(df.columns.values))\n",
    "#print(df.columns.name)\n",
    "x=(list(df.columns.values))\n",
    "print(x)\n",
    "print(\"___________--\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c=df\n",
    "c.head()\n",
    "\n",
    "a=c.groupby('Col1').count()\n",
    "print(a)\n",
    "print(\"aaa\")\n",
    "x=a.nlargest(100,['Col2'])\n",
    "print(x)\n",
    "#y=x.index.values\n",
    "x['A'] = x.index\n",
    "print(x['Col2'])\n",
    "x = x[['A','Col2']]\n",
    "print(list(x.columns.values))\n",
    "np.savetxt(path+'\\influencer.txt', x.values, fmt='%d', delimiter='\\t')#c= pandas.write_table('C:\\\\Users\\\\priya\\\\OneDrive\\\\Desktop\\\\Friends Reccomendation\\\\Friends-Recommender-In-Social-Network-master\\\\influencer.txt', delim_whitespace=True, names=('A', 'B'),dtype={'A': np.int64, 'B': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CommonNeighbors(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    return len(u_neighbors.intersection(v_neighbors))\n",
    "def common_neighbors(g, edges):\n",
    "    result = []\n",
    "    for edge in edges:\n",
    "        node_one, node_two = edge[0], edge[1]\n",
    "        num_common_neighbors = 0\n",
    "        try:\n",
    "            neighbors_one, neighbors_two = g.neighbors(node_one), g.neighbors(node_two)\n",
    "            for neighbor in neighbors_one:\n",
    "                if neighbor in neighbors_two:\n",
    "                    num_common_neighbors += 1\n",
    "            result.append((node_one, node_two, num_common_neighbors))\n",
    "        except:\n",
    "            pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamicAdar(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    aa = 0\n",
    "    for i in u_neighbors.intersection(v_neighbors):\n",
    "        aa += 1 / math.log(len(g.neighbors(i)))\n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResourceAllocation(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    ra = 0\n",
    "    for i in u_neighbors.intersection(v_neighbors):\n",
    "        ra += 1 / float(len(g.neighbors(i)))\n",
    "    return ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JaccardCoefficent(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    return len(u_neighbors.intersection(v_neighbors)) / float(len(u_neighbors.union(v_neighbors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreferentialAttachment(u, v, g):\n",
    "    return len(g.neighbors(u))*len(g.neighbors(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_feature(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    print(u_neighbors)\n",
    "    print(v_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AllFeatures(u,v,g1, g2):\n",
    "    '''\n",
    "    the change of features in two consecutive sub graphs\n",
    "    '''\n",
    "    try:\n",
    "        cn = CommonNeighbors(u, v, g2)\n",
    "        aa = AdamicAdar(u, v, g2)\n",
    "        ra = ResourceAllocation(u, v, g2)\n",
    "        jc = JaccardCoefficent(u, v, g2)\n",
    "        pa = PreferentialAttachment(u, v, g2)\n",
    "\n",
    "        delta_cn = cn - CommonNeighbors(u, v, g1)\n",
    "        delta_aa = aa - AdamicAdar(u, v, g1)\n",
    "        delta_ra = ra - ResourceAllocation(u, v, g1)\n",
    "        delta_jc = jc - JaccardCoefficent(u, v, g1)\n",
    "        delta_pa = pa - PreferentialAttachment(u, v, g1)\n",
    "        return {\"cn\":cn, \"aa\": aa, \"ra\":ra, \"jc\":jc, \"pa\":pa,\n",
    "            \"delta_cn\": delta_cn, \"delta_aa\": delta_aa, \"delta_ra\": delta_ra,\n",
    "             \"delta_jc\": delta_jc, \"delta_pa\": delta_pa}\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = [common_neighbors,\n",
    "                   nx.resource_allocation_index,\n",
    "                   nx.jaccard_coefficient,\n",
    "                   nx.adamic_adar_index,\n",
    "                   nx.preferential_attachment\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_fake_edge(g, neg_g,num_test_edges):\n",
    "    i = 0\n",
    "    while i < num_test_edges:\n",
    "        edge = random.sample(g.nodes(), 2)\n",
    "        try:\n",
    "            shortest_path = nx.shortest_path_length(g,source=edge[0],target=edge[1])\n",
    "            if shortest_path >= 2:\n",
    "                neg_g.add_edge(edge[0],edge[1], positive=\"False\")\n",
    "                i += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_file(filename):\n",
    "    print(\"----------------build graph--------------------\")\n",
    "    f = open(filename, \"rb\")\n",
    "    g = nx.read_edgelist(f)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_extraction(g, pos_num, neg_num, neg_mode, neg_distance=2, delete=1):\n",
    "    \"\"\"\n",
    "\n",
    "    :param g:  the graph\n",
    "    :param pos_num: the number of positive samples\n",
    "    :param neg_num: the number of negative samples\n",
    "    :param neg_distance: the distance between two nodes in negative samples\n",
    "    :param delete: if delete ==0, don't delete positive edges from graph\n",
    "    :return: pos_sample is a list of positive edges, neg_sample is a list of negative edges\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----------------extract positive samples--------------------\")\n",
    "    # randomly select pos_num as test edges\n",
    "    pos_sample = random.sample(g.edges(), pos_num)\n",
    "    sample_g = nx.Graph()\n",
    "    sample_g.add_edges_from(pos_sample, positive=\"True\")\n",
    "    nx.write_edgelist(sample_g, \"sample_positive_\" +str(pos_num)+ \".txt\", data=['positive'])\n",
    "\n",
    "    # adding non-existing edges\n",
    "    print(\"----------------extract negative samples--------------------\")\n",
    "    i = 0\n",
    "    neg_g = nx.Graph()\n",
    "    produce_fake_edge(g,neg_g,neg_num)\n",
    "    nx.write_edgelist(neg_g, \"sample_negative_\" +str(neg_num)+ \".txt\", data=[\"positive\"])\n",
    "    neg_sample = neg_g.edges()\n",
    "    neg_g.add_edges_from(pos_sample,positive=\"True\")\n",
    "    nx.write_edgelist(neg_g, \"sample_combine_\" +str(pos_num + neg_num)+ \".txt\", data=[\"positive\"])\n",
    "\n",
    "    # remove the positive sample edges, the rest is the training set\n",
    "    if delete == 0:\n",
    "        return pos_sample, neg_sample\n",
    "    else:\n",
    "        g.remove_edges_from(pos_sample)\n",
    "        nx.write_edgelist(g, \"training.txt\", data=False)\n",
    "\n",
    "        return pos_sample, neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(g, pos_sample, neg_sample, feature_name, model=\"single\", combine_num=5):\n",
    "\n",
    "    data = []\n",
    "    if model == \"single\":\n",
    "        print (\"-----extract feature:\", feature_name.__name__, \"----------\")\n",
    "        preds = feature_name(g, pos_sample)\n",
    "        feature = [feature_name.__name__] + [i[2] for i in preds]\n",
    "        label = [\"label\"] + [\"Pos\" for i in range(len(feature))]\n",
    "        preds = feature_name(g, neg_sample)\n",
    "        feature1 = [i[2] for i in preds]\n",
    "        feature = feature + feature1\n",
    "        label = label + [\"Neg\" for i in range(len(feature1))]\n",
    "        data = [feature, label]\n",
    "        data = transpose(data)\n",
    "        print(\"----------write the feature to file---------------\")\n",
    "        write_data_to_file(data, \"features_\" + model + \"_\" + feature_name.__name__ + \".csv\")\n",
    "    else:\n",
    "        label = [\"label\"] + [\"1\" for i in range(len(pos_sample))] + [\"0\" for i in range(len(neg_sample))]\n",
    "        for j in feature_name:\n",
    "            print (\"-----extract feature:\", j.__name__, \"----------\")\n",
    "            preds = j(g, pos_sample)\n",
    "\n",
    "            feature = [j.__name__] + [i[2] for i in preds]\n",
    "            preds = j(g, neg_sample)\n",
    "            feature = feature + [i[2] for i in preds]\n",
    "            data.append(feature)\n",
    "\n",
    "        data.append(label)\n",
    "        data = transpose(data)\n",
    "        print(\"----------write the features to file---------------\")\n",
    "        write_data_to_file(data, \"features_\" + model + \"_\" + str(combine_num) + \".csv\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_data_to_file(data, filename):\n",
    "    csvfile = open(filename, \"w\")\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in data:\n",
    "        writer.writerow(i)\n",
    "    csvfile.close()\n",
    "\n",
    "\n",
    "def transpose(data):\n",
    "    return [list(i) for i in zip(*data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filename=\"facebook_combined.txt\", pos_num=0.1, neg_num=0.1, model=\"combined\", combine_num=1,\n",
    "         feature_name=common_neighbors, neg_mode=\"hard\"):\n",
    "    if combine_num==2:\n",
    "        pos_num=0.008;\n",
    "        neg_num=0.008;\n",
    "    g = create_graph_from_file(filename)\n",
    "    num_edges = g.number_of_edges()\n",
    "    pos_num = int(num_edges * pos_num)\n",
    "    neg_num = int(num_edges * neg_num)\n",
    "    pos_sample, neg_sample = sample_extraction(g, pos_num, neg_num,neg_mode)\n",
    "    train_data = feature_extraction(g, pos_sample, neg_sample, feature_name, model, combine_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#______________________Entry Point________________________\n",
    "#Fn: Name of data set you want to run this code for, and cn is a integer for that dataset(any integer will work but different for each dataset)\n",
    "#By default it is set to Twitter Data Set\n",
    "#The project was run using Facebook and Twitter dataset but it works with any social network dataset from http://snap.stanford.edu/data/\n",
    "#Following Scoring Methods are used to construct feature Set----\n",
    "#common_neighbors,resource_allocation_index, jaccard_coefficient, adamic_adar_index, preferential_attachment\n",
    "#SVM ANN and Logistic Regresssion is used for classificaion\n",
    "fn=\"facebook_combined.txt\";\n",
    "cn=2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------build graph--------------------\n",
      "----------------extract positive samples--------------------\n",
      "----------------extract negative samples--------------------\n",
      "('-----extract feature:', 'common_neighbors', '----------')\n",
      "('-----extract feature:', 'resource_allocation_index', '----------')\n",
      "('-----extract feature:', 'jaccard_coefficient', '----------')\n",
      "('-----extract feature:', 'adamic_adar_index', '----------')\n",
      "('-----extract feature:', 'preferential_attachment', '----------')\n",
      "----------write the features to file---------------\n"
     ]
    }
   ],
   "source": [
    "#Run this line to genrate feature Set\n",
    "main(filename=fn,model=\"combined\",combine_num=cn, feature_name=feature_set, neg_mode=\"easy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=np.loadtxt(open(\"features_combined_\"+str(cn)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,b=r.shape;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l=int(0.8*l)\n",
    "X_train=r[0:train_l,0:b-1]\n",
    "Y_train=r[0:train_l,b-1]\n",
    "X_test=r[train_l:l,0:b-1]\n",
    "Y_test=r[train_l:l,b-1]\n",
    "# X_train = normalize(X_train, axis=0, norm='max')\n",
    "# X_test = normalize(X_test, axis=0, norm='max')\n",
    "# scaler = StandardScaler()  \n",
    "# scaler.fit(X_train)  \n",
    "# X_train = scaler.transform(X_train)  \n",
    "# X_test = scaler.transform(X_test)  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "def DecisionTree_Train(training, training_labels, testing, testing_labels):\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     from sklearn import preprocessing\n",
    "#     X_scaled = preprocessing.scale(training)\n",
    "#     scaler = preprocessing.StandardScaler().fit(training)\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     training = min_max_scaler.fit_transform(training)\n",
    "#     testing = min_max_scaler.transform(testing)\n",
    "    dtree_clf = DecisionTreeClassifier(random_state=1)\n",
    "    dtree_clf.fit(training, training_labels)\n",
    "    train_pred = dtree_clf.predict(training)\n",
    "    train_score = accuracy_score(training_labels, train_pred)\n",
    "#     start = datetime.datetime.now()\n",
    "#     clf.fit(training, training_labels)\n",
    "#     print (\"+++++++++ Finishing training the ANN classifier ++++++++++++\")\n",
    "#     result = clf.predict(testing)\n",
    "    return train_score, dtree_clf\n",
    "#     print (\"Decision accuracy:\", accuracy_score(testing_labels, result))\n",
    "#     #keep the time\n",
    "#     finish = datetime.datetime.now()\n",
    "#     print ((finish-start).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RandomForest_Train(training, training_labels, testing, testing_labels):\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     from sklearn import preprocessing\n",
    "#     X_scaled = preprocessing.scale(training)\n",
    "#     scaler = preprocessing.StandardScaler().fit(training)\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     training = min_max_scaler.fit_transform(training)\n",
    "#     testing = min_max_scaler.transform(testing)\n",
    "    rFor_clr = RandomForestClassifier(n_estimators = 2000, random_state=1, class_weight = 'balanced_subsample')\n",
    "#     start = datetime.datetime.now()\n",
    "    rFor_clr.fit(training, training_labels)\n",
    "    train_pred = rFor_clr.predict(training)\n",
    "    train_score = accuracy_score(training_labels, train_pred)\n",
    "    return train_score, rFor_clr\n",
    "#     print (\"+++++++++ Finishing training the Random Forest classifier ++++++++++++\")\n",
    "#     result = clf.predict(testing)\n",
    "\n",
    "#     print (\"Random Forest accuracy:\", accuracy_score(testing_labels, result))\n",
    "#     #keep the time\n",
    "#     finish = datetime.datetime.now()\n",
    "#     print ((finish-start).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7039007092198581,\n",
       " SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=7, gamma='auto', kernel='poly',\n",
       "   max_iter=10000000, probability=False, random_state=10, shrinking=True,\n",
       "   tol=0.001, verbose=False))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def svm_Train(training, training_labels, testing, testing_labels):\n",
    "    #Support Vector Machine\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn import preprocessing\n",
    "    training = normalize(training, axis=0, norm='max')\n",
    "    testing = normalize(testing, axis=0, norm='max')\n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(training)  \n",
    "    training = scaler.transform(training)  \n",
    "    testing = scaler.transform(testing) \n",
    "#     X_scaled = preprocessing.scale(training)\n",
    "#     scaler = preprocessing.StandardScaler().fit(training)\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     training = min_max_scaler.fit_transform(training)\n",
    "#     testing = min_max_scaler.transform(testing)\n",
    "    start = datetime.datetime.now()\n",
    "    svm_clf = svm.SVC(kernel = 'poly', degree = 7, class_weight = 'balanced', max_iter = 10000000, random_state = 10)\n",
    "    svm_clf.fit(training, training_labels)\n",
    "    train_pred = svm_clf.predict(training)\n",
    "    train_score = accuracy_score(training_labels, train_pred)\n",
    "    return train_score, svm_clf\n",
    "#train_pred, train_score\n",
    "svm_Train(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_Train(training, training_labels, testing, testing_labels):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn import preprocessing\n",
    "#     X_scaled = preprocessing.scale(training)\n",
    "#     scaler = preprocessing.StandardScaler().fit(training)\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     training = min_max_scaler.fit_transform(training)\n",
    "#     testing = min_max_scaler.transform(testing)\n",
    "    log_clf = LogisticRegression(random_state=1, solver='saga',multi_class='multinomial', class_weight = 'balanced', max_iter = 1000, warm_start = True).fit(training, training_labels)\n",
    "#     start = datetime.datetime.now()\n",
    "    var = log_clf.fit(training, training_labels)\n",
    "    train_pred = log_clf.predict(training)\n",
    "    train_score = accuracy_score(training_labels, train_pred)\n",
    "    return train_score, log_clf\n",
    "#train_pred, train_score\n",
    "# logistic_Train(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9778368794326241,\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knn_Train(training, training_labels, testing, testing_labels):\n",
    "    \n",
    "    knn_clr = KNeighborsClassifier()\n",
    "    knn_clr.fit(training, training_labels)\n",
    "    train_pred = knn_clr.predict(training)\n",
    "    train_score = accuracy_score(training_labels, train_pred)\n",
    "    return train_score, knn_clr\n",
    "knn_Train(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.949468085106383, GaussianNB(priors=None))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "def naive_Train(training, training_labels, testing, testing_labels):\n",
    "    naive_clr = GaussianNB()\n",
    "    naive_clr.fit(training, training_labels)\n",
    "    train_pred = naive_clr.predict(training)\n",
    "    train_score = accuracy_score(training_labels, train_pred)\n",
    "#     test_pred = naive_clr.predict(data)\n",
    "#     test_score = naive_clr.score(testing, testing_labels)\n",
    "    return train_score, naive_clr\n",
    "naive_Train(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9769503546099291,\n",
       " MLPClassifier(activation='relu', alpha=1e-09, batch_size='auto', beta_1=0.9,\n",
       "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "        hidden_layer_sizes=(15, 9), learning_rate='constant',\n",
       "        learning_rate_init=0.001, max_iter=100000, momentum=0.9,\n",
       "        nesterovs_momentum=True, power_t=0.5, random_state=100,\n",
       "        shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "        verbose=False, warm_start=True))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ANN_Train(training, training_labels, testing, testing_labels):\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     from sklearn import preprocessing\n",
    "#     X_scaled = preprocessing.scale(training)\n",
    "#     scaler = preprocessing.StandardScaler().fit(training)\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     training = min_max_scaler.fit_transform(training)\n",
    "#     testing = min_max_scaler.transform(testing)  \n",
    "    ann_clf = MLPClassifier(solver='lbfgs', alpha=1e-9,hidden_layer_sizes=(15,9), random_state=100, max_iter=100000, warm_start = True)\n",
    "#     start = datetime.datetime.now()\n",
    "    ann_clf.fit(training, training_labels)\n",
    "    train_pred = ann_clf.predict(training)\n",
    "    train_score = accuracy_score(training_labels, train_pred)\n",
    "    return train_score, ann_clf\n",
    "ANN_Train(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SVM_score:', 0.7039007092198581)\n",
      "('NAIVE_score:', 0.949468085106383)\n",
      "('ANN_score:', 0.9769503546099291)\n",
      "('LOG_score:', 0.8687943262411347)\n",
      "('Ensemble_score:', 0.9414211674849973)\n",
      "0.5070921985815603\n",
      "0.8581560283687943\n",
      "0.9716312056737588\n",
      "0.975177304964539\n",
      "0.9539007092198581\n",
      "0.9680851063829787\n",
      "0.9716312056737588\n",
      "0.8617021276595744\n"
     ]
    }
   ],
   "source": [
    "#Ensemble of ANN, Logistic, svm\n",
    "from scipy import stats\n",
    "#Get the train-set accuracy scores and the train data fitted classifiers\n",
    "svm_score, svm_fitted = svm_Train(X_train,Y_train,X_test,Y_test)\n",
    "log_score, log_fitted = logistic_Train(X_train,Y_train,X_test,Y_test)\n",
    "ann_score, ann_fitted = ANN_Train(X_train,Y_train,X_test,Y_test)\n",
    "dtree_score, dtree_fitted = DecisionTree_Train(X_train,Y_train,X_test,Y_test)\n",
    "rfor_score, rfor_fitted = RandomForest_Train(X_train,Y_train,X_test,Y_test)\n",
    "knn_score, knn_fitted = knn_Train(X_train,Y_train,X_test,Y_test)\n",
    "naive_score, naive_fitted = naive_Train(X_train,Y_train,X_test,Y_test)\n",
    "print(\"SVM_score:\",svm_score)\n",
    "print(\"NAIVE_score:\",naive_score)\n",
    "print(\"ANN_score:\",ann_score)\n",
    "print(\"LOG_score:\",log_score)\n",
    "training_score = []\n",
    "training_score.append(float(ann_score))\n",
    "training_score.append(float(naive_score))\n",
    "training_score.append(float(svm_score))\n",
    "training_score.append(float(log_score))\n",
    "\n",
    "ensemble_score = (5 * float(ann_score) + 5 * float(naive_score) + 3 * float(log_score) ) /13\n",
    "training_score.append(ensemble_score)\n",
    "print( \"Ensemble_score:\",ensemble_score)\n",
    "pred = []\n",
    "log_allPred = []\n",
    "ann_allPred = []\n",
    "svm_allPred = []\n",
    "knn_allPred = []\n",
    "naive_allPred = []\n",
    "dtree_allPred = []\n",
    "rfor_allPred = []\n",
    "\n",
    "for data in X_test:\n",
    "    data = data.reshape(1, -1)\n",
    "    #Predict test set with the fited classifers\n",
    "    svm_pred = svm_fitted.predict(data)\n",
    "    svm_allPred.append(svm_pred)\n",
    "    log_pred = log_fitted.predict(data)\n",
    "    log_allPred.append(log_pred)\n",
    "    ann_pred = ann_fitted.predict(data)\n",
    "    ann_allPred.append(ann_pred)\n",
    "    knn_pred = knn_fitted.predict(data)\n",
    "    knn_allPred.append(knn_pred)\n",
    "    naive_pred = naive_fitted.predict(data)\n",
    "    naive_allPred.append(naive_pred)\n",
    "    dtree_pred = dtree_fitted.predict(data)\n",
    "    dtree_allPred.append(dtree_pred)\n",
    "    rfor_pred = rfor_fitted.predict(data)\n",
    "    rfor_allPred.append(rfor_pred)\n",
    "    #pred.append(stats.mode(np.array([int(svm_pred), int(log_pred), int(ann_pred)]))[0][0])\n",
    "    \n",
    "    #Weighted Voting -  Ensemble method\n",
    "    value = (5 * int(ann_pred) + 5 * int(naive_pred)  + 3 * int(log_pred) ) /13\n",
    "    if value >= 0.5:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "\n",
    "print(accuracy_score(svm_allPred, Y_test))\n",
    "print(accuracy_score(log_allPred, Y_test))\n",
    "print(accuracy_score(ann_allPred, Y_test))\n",
    "print(accuracy_score(knn_allPred, Y_test))\n",
    "print(accuracy_score(naive_allPred, Y_test))\n",
    "print(accuracy_score(dtree_allPred, Y_test))\n",
    "print(accuracy_score(rfor_allPred, Y_test))\n",
    "print(accuracy_score(pred, Y_test))\n",
    "testing_score = []\n",
    "testing_score.append(accuracy_score(ann_allPred, Y_test))\n",
    "testing_score.append(accuracy_score(naive_allPred, Y_test))\n",
    "testing_score.append(accuracy_score(svm_allPred, Y_test))\n",
    "testing_score.append(accuracy_score(log_allPred, Y_test))\n",
    "testing_score.append(accuracy_score(pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score.append(dtree_score)\n",
    "training_score.append(rfor_score)\n",
    "training_score.append(knn_score)\n",
    "testing_score.append(accuracy_score(dtree_allPred, Y_test))\n",
    "testing_score.append(accuracy_score(rfor_allPred, Y_test))\n",
    "testing_score.append(accuracy_score(knn_allPred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9769503546099291, 0.949468085106383, 0.7039007092198581, 0.8687943262411347, 0.9414211674849973, 1.0, 1.0, 0.9778368794326241]\n",
      "[0.9716312056737588, 0.9539007092198581, 0.5070921985815603, 0.8581560283687943, 0.8617021276595744, 0.9680851063829787, 0.9716312056737588, 0.975177304964539]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3d4111a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_axis = np.arange(0, 101)\n",
    "labels = ['ANN', 'NAIVE', 'SVM', 'LOG', 'Ensemble', 'D. Tree', 'R. Forest', 'KNN']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.5\n",
    "print(training_score)\n",
    "print(testing_score)\n",
    "plt.plot(training_score, label = 'Training Scores')\n",
    "plt.plot(testing_score, label = 'Testing Scores')\n",
    "# plt.bar(x-width/2, training_score, width = width, label = 'Training Scores')\n",
    "# plt.bar(x+width/2, testing_score,width = width, label = 'Testing Scores')\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
